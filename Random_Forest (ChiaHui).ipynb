{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "285997a8",
   "metadata": {},
   "source": [
    "## Random Forest (ChiaHui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "013ee5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77d5eaa",
   "metadata": {},
   "source": [
    "## Part A - Model Variety Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c77eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dff5ce",
   "metadata": {},
   "source": [
    "#### Import Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e2e7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(\"C:\\Datasets\\Crime_Data_from_2020_to_Present.csv\", low_memory=\"False\")\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cefa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c40a811",
   "metadata": {},
   "source": [
    "### Data Preparetion \n",
    "#### Map each Crime commited to a maytching criminal offense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bc6213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keyword-based mapping rules for auto‐labeling\n",
    "mapping_rules = {\n",
    "    \"Violent Crime\": [\n",
    "        \"ASSAULT\", \"BATTERY\", \"HOMICIDE\", \"MANSLAUGHTER\", \"RAPE\",\n",
    "        \"SEXUAL\", \"SODOMY\", \"ORAL COPULATION\", \"KIDNAPPING\",\n",
    "        \"LYNCHING\", \"STALKING\", \"THREATS\", \"INTIMATE PARTNER\"\n",
    "    ],\n",
    "    \"Property Crime\": [\n",
    "        \"THEFT\", \"BURGLARY\", \"VANDALISM\", \"ARSON\", \"SHOPLIFTING\",\n",
    "        \"BIKE - STOLEN\", \"COIN MACHINE\"\n",
    "    ],\n",
    "    \"Vehicle Crime\": [\n",
    "        \"VEHICLE\", \"DRIVING WITHOUT OWNER CONSENT\", \"DWOC\"\n",
    "    ],\n",
    "    \"Fraud / Financial Crime\": [\n",
    "        \"FRAUD\", \"EMBEZZLEMENT\", \"COUNTERFEIT\", \"BUNCO\",\n",
    "        \"CREDIT CARD\", \"DOCUMENT WORTHLESS\", \"INSURANCE\"\n",
    "    ],\n",
    "    \"Weapons / Public Safety\": [\n",
    "        \"FIREARM\", \"WEAPON\", \"SHOTS FIRED\", \"BOMB\", \"BRANDISH\"\n",
    "    ],\n",
    "    \"Sex Crime\": [\n",
    "        \"LEWD\", \"INDECENT EXPOSURE\", \"CHILD PORNOGRAPHY\",\n",
    "        \"PANDERING\", \"PIMPING\", \"HUMAN TRAFFICKING\"\n",
    "    ],\n",
    "    \"Child-Related Crime\": [\n",
    "        \"CHILD\", \"CONTRIBUTING\", \"CHILD NEGLECT\"\n",
    "    ],\n",
    "    \"Court / Restraining Order / Legal\": [\n",
    "        \"COURT\", \"RESTRAINING\", \"CONTEMPT\", \"FAILURE TO APPEAR\",\n",
    "        \"VIOLATION\"\n",
    "    ],\n",
    "    \"Public Disturbance / Disorder\": [\n",
    "        \"DISTURBANCE\", \"PEACE\", \"TRESPASS\", \"DISRUPT\",\n",
    "        \"RIOT\", \"DISOBEY\"\n",
    "    ],\n",
    "    \"Other Crime\": []  # fallback\n",
    "}\n",
    "\n",
    "# Function to classify crimes\n",
    "def classify(description: str):\n",
    "    if not isinstance(description, str):\n",
    "        return \"Other Crime\"\n",
    "    desc = description.upper()\n",
    "    for category, keywords in mapping_rules.items():\n",
    "        for kw in keywords:\n",
    "            if kw in desc:\n",
    "                return category\n",
    "    return \"Other Crime\"\n",
    "\n",
    "# Create new class column\n",
    "df_raw[\"Crime_Class\"] = df_raw[\"Crm Cd Desc\"].apply(classify)\n",
    "\n",
    "# Save a preview\n",
    "preview = df_raw[[\"Crm Cd Desc\", \"Crime_Class\"]].head(30)\n",
    "preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4384be9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_raw['Crime_Class'].value_counts().reset_index(name='Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7680bbd",
   "metadata": {},
   "source": [
    "### Category: Machine Learning Models\n",
    "\n",
    "#### Tree Based: Random Forest Tree (RAW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b16673",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Random Forest Crime Classification ===\")\n",
    "\n",
    "# Remove Crm Cd Desc to avoid leakage\n",
    "df_model = df_raw.drop(columns=[\n",
    "    \"Crm Cd Desc\", \"Crm Cd\", \"Premis Cd\", \"Premis Desc\",\n",
    "    \"Crm Cd 1\", \"Crm Cd 2\", \"Crm Cd 3\", \"Crm Cd 4\"\n",
    "])\n",
    "\n",
    "# Prepare training data\n",
    "X = df_model.drop(columns=[\"Crime_Class\"])\n",
    "y, class_names = pd.factorize(df_model[\"Crime_Class\"])\n",
    "\n",
    "# Convert datetime columns to int timestamps\n",
    "for col in X.select_dtypes(include=['datetime', 'datetimetz']).columns:\n",
    "    X[col] = X[col].view('int64')\n",
    "\n",
    "# Factorize object columns\n",
    "X = X.apply(lambda col: pd.factorize(col)[0] if col.dtype == \"object\" else col)\n",
    "\n",
    "# Train-test split (stratify for class balance)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Initialize Random Forest with balanced, regularized params\n",
    "rf_model_1 = RandomForestClassifier()\n",
    "# Save paraneter tunning for PART B. \n",
    "\n",
    "rf_model_1.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_test_1 = rf_model_1.predict(X_test)\n",
    "y_pred_train_1 = rf_model_1.predict(X_train)\n",
    "\n",
    "# Accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train_1)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test_1)\n",
    "\n",
    "print(\"\\n=== Performance ===\")\n",
    "print(f\"Training Set Accuracy: {train_accuracy}\")\n",
    "print(f\"Testing Set Accuracy:  {test_accuracy}\")\n",
    "print(f\"Overfit Gap:          {train_accuracy - test_accuracy:.4f}\")\n",
    "\n",
    "# Evaluation reports\n",
    "report_test = pd.DataFrame.from_dict(\n",
    "    classification_report(y_test, y_pred_test_1, output_dict=True)\n",
    ").transpose()\n",
    "\n",
    "report_train = pd.DataFrame.from_dict(\n",
    "    classification_report(y_train, y_pred_train_1, output_dict=True)\n",
    ").transpose()\n",
    "\n",
    "print(\"------------------------------------------------------------------------------------\")\n",
    "print(\"Training Set Report\")\n",
    "print(\"------------------------------------------------------------------------------------\")\n",
    "print(report_train)\n",
    "print(\"------------------------------------------------------------------------------------\")\n",
    "print(\"Testing Set Report\")\n",
    "print(\"------------------------------------------------------------------------------------\")\n",
    "print(report_test)\n",
    "print(\"------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb4532f",
   "metadata": {},
   "source": [
    "### Part A - Feature Engineering and Transformation\n",
    "\n",
    "#### Data Cleaning (Check for duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb1f9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_raw.drop_duplicates()\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98dc381",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.drop(columns=[\"Crm Cd Desc\", \"Crm Cd\", \"Premis Cd\", \"Premis Desc\", \"Crm Cd 1\", \"Crm Cd 2\", \"Crm Cd 3\", \"Crm Cd 4\"])\n",
    "df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97f1398",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808740c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Clean DATE OCC (mixed formats)\n",
    "df_new['DATE OCC'] = pd.to_datetime(df_new['DATE OCC'], format='mixed', errors='coerce')\n",
    "\n",
    "# 2. Clean TIME OCC (force numeric → Int64 → 4-digit HHMM)\n",
    "df_new['TIME OCC'] = pd.to_numeric(df_new['TIME OCC'], errors='coerce').astype('Int64')\n",
    "time_str = df_new['TIME OCC'].astype(str).str.zfill(4)\n",
    "\n",
    "# 3. Combine DATE OCC + TIME OCC into a single datetime\n",
    "df_new['DateTime OCC'] = pd.to_datetime(\n",
    "    df_new['DATE OCC'].dt.strftime('%Y-%m-%d') + ' ' + time_str,\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# 4. Drop the original columns used for merging\n",
    "df_new = df_new.drop(columns=['DATE OCC', 'TIME OCC'])\n",
    "\n",
    "df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c0563c",
   "metadata": {},
   "source": [
    "### Check Null Value \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb3df0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b64c14",
   "metadata": {},
   "source": [
    "Remove the Weapon Used Cd column, and change the Weapon Desc column to binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd25974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Drop the Weapon Used Cd column (if it exists)\n",
    "df_new = df_new.drop(columns=['Weapon Used Cd'], errors='ignore')\n",
    "\n",
    "# 2. Create a binary Weapon_Present column\n",
    "df_new['Weapon_Present'] = df_new['Weapon Desc'].apply(\n",
    "    lambda x: 'Present' if pd.notna(x) and str(x).strip() != '' else 'Absent'\n",
    ")\n",
    "\n",
    "# 3. (Optional) Drop Weapon Desc if you want to fully remove the text info\n",
    "df_new = df_new.drop(columns=['Weapon Desc'], errors='ignore')\n",
    "\n",
    "df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01685067",
   "metadata": {},
   "source": [
    "Dropping columns that provide meaningless value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e94135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "sns.heatmap(df_new.isna(), cbar=False, yticklabels=False)\n",
    "plt.title(\"Missing Value Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d3dd0c",
   "metadata": {},
   "source": [
    "### Test Pearson Correlation (Numeric Features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca35840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the DataFrame to analyze; use the most recent processed one if available\n",
    "try:\n",
    "    df_corr_source = df_new.copy()\n",
    "except NameError:\n",
    "    df_corr_source = df_raw.copy()\n",
    "\n",
    "# Ensure target encoding (optional): demonstrate correlation against encoded target when present\n",
    "if 'Crime_Class' in df_corr_source.columns:\n",
    "    df_corr_source['Crime_Class_numeric'] = df_corr_source['Crime_Class'].astype('category').cat.codes\n",
    "\n",
    "# Select only numeric columns\n",
    "num_df = df_corr_source.select_dtypes(include=['number'])\n",
    "\n",
    "# Pearson correlation matrix\n",
    "corr = num_df.corr(numeric_only=True)\n",
    "\n",
    "# Upper triangle flatten for pairwise sorted report\n",
    "upper = corr.where(~np.tril(np.ones(corr.shape)).astype(bool))\n",
    "corr_report = (\n",
    "    upper.stack()\n",
    "          .reset_index()\n",
    "          .rename(columns={'level_0': 'Feature 1', 'level_1': 'Feature 2', 0: 'Correlation'})\n",
    ")\n",
    "\n",
    "# Sort by absolute correlation strength\n",
    "corr_report = corr_report.iloc[corr_report['Correlation'].abs().sort_values(ascending=False).index]\n",
    "\n",
    "# Show top pairs\n",
    "print(\"Top 25 strongest Pearson correlations (absolute):\")\n",
    "print(corr_report.head(25))\n",
    "\n",
    "# Optional: heatmap for a quick visual\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr, cmap='coolwarm', center=0)\n",
    "plt.title('Pearson Correlation (Numeric Features)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb417a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns\n",
    "num_df_new = df_new.select_dtypes(include=['number'])\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr = num_df_new.corr(numeric_only=True)\n",
    "\n",
    "# Turn it into a sorted report (pairwise correlations)\n",
    "corr_report = (\n",
    "    corr.where(~np.tril(np.ones(corr.shape)).astype(bool))  # keep upper triangle\n",
    "        .stack()\n",
    "        .reset_index()\n",
    ")\n",
    "corr_report.columns = [\"Feature 1\", \"Feature 2\", \"Correlation\"]\n",
    "\n",
    "# Sort by absolute correlation strength\n",
    "corr_report = corr_report.iloc[corr_report['Correlation'].abs().sort_values(ascending=False).index]\n",
    "\n",
    "corr_report.head(20)   # view top 20 strongest relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d868c132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the data\n",
    "df_corr = df_new.copy()\n",
    "\n",
    "# Convert Crime_Class (categorical) → numeric labels\n",
    "df_corr['Crime_Class_numeric'] = df_corr['Crime_Class'].astype('category').cat.codes\n",
    "\n",
    "# Select only numeric columns\n",
    "num_df = df_corr.select_dtypes(include=['number'])\n",
    "\n",
    "# Compute correlation with the numeric-encoded target\n",
    "target_corr = num_df.corr(numeric_only=True)['Crime_Class_numeric']\n",
    "\n",
    "# Remove the target itself\n",
    "target_corr = target_corr.drop(labels=['Crime_Class_numeric'])\n",
    "\n",
    "# Turn into sorted dataframe\n",
    "target_corr_report = (\n",
    "    target_corr\n",
    "        .abs()\n",
    "        .sort_values(ascending=False)\n",
    "        .rename(\"Correlation_with_Crime_Class\")\n",
    "        .to_frame()\n",
    ")\n",
    "\n",
    "target_corr_report.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91bea66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd753e69",
   "metadata": {},
   "source": [
    "### Mocodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c76bc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Clean & explode the MO Codes column ---\n",
    "# Convert NaN to empty string\n",
    "df_new['Mocodes'] = df_new['Mocodes'].fillna('')\n",
    "\n",
    "# Split by spaces → expand into list\n",
    "df_new['MOCODES_LIST'] = df_new['Mocodes'].str.strip().str.split()\n",
    "\n",
    "# Explode (each code becomes a row)\n",
    "exploded = df_new.explode('MOCODES_LIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4163c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_codes = sorted({code for sublist in df_new['MOCODES_LIST'] for code in sublist})\n",
    "print(len(all_codes), \"unique MO codes found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525f29e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Count MO code frequencies ---\n",
    "mo_counts = (\n",
    "    exploded['MOCODES_LIST']\n",
    "    .value_counts()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9619a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Select the Top 100 codes ---\n",
    "top_100 = set(mo_counts.head(100).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543c7908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Create one-hot columns for each top code ---\n",
    "for code in top_100:\n",
    "    df_new[f\"MO_{code}\"] = df_new['MOCODES_LIST'].apply(lambda lst: code in lst)\n",
    "\n",
    "# --- Step 5: Create the OTHERS column ---\n",
    "# OTHERS = true if the row contains any MO code NOT in the top 100\n",
    "df_new['MO_OTHERS'] = df_new['MOCODES_LIST'].apply(\n",
    "    lambda lst: any(code not in top_100 for code in lst)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3916962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 6: Convert booleans to integers (0/1) ---\n",
    "mo_cols = [col for col in df_new.columns if col.startswith(\"MO_\")]\n",
    "df_new[mo_cols] = df_new[mo_cols].astype(int)\n",
    "\n",
    "# --- Step 7: Clean up temporary column ---\n",
    "df_new_1 = df_new.drop(columns=[\"MOCODES_LIST\"])\n",
    "\n",
    "# --- Done ---\n",
    "print(f\"Created {len(mo_cols)} MO Code features (100 Top + OTHERS).\")\n",
    "print(mo_cols[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa4a936",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_1 = df_new_1.drop(columns=['Mocodes'], errors='ignore')\n",
    "df_new_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80412b4",
   "metadata": {},
   "source": [
    "### Location Baesd Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d51c988",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_1 = df_new_1.drop(columns=['LOCATION', 'Cross Street', 'DateTime OCC', 'Date Rptd'], errors='ignore') #Remove the dates\n",
    "df_new_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d726c1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_1 = df_new_1.drop(columns=['AREA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13716415",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_1 = df_new_1.drop(columns=['AREA NAME'])\n",
    "df_new_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df29a19",
   "metadata": {},
   "source": [
    "### LON & LAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ee409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "coords = df_new_1[['LAT', 'LON']].dropna()\n",
    "\n",
    "kmeans = KMeans(n_clusters=100, random_state=42)\n",
    "\n",
    "df_new_1['Location_Cluster'] = kmeans.fit_predict(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5162442",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4b700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_1['Lat_bin'] = pd.cut(df_new_1['LAT'], bins=50, labels=False)\n",
    "df_new_1['Lon_bin'] = pd.cut(df_new_1['LON'], bins=50, labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10d0a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_new_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4568b1",
   "metadata": {},
   "source": [
    "## Remodelling with Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6597a8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Random Forest Crime Classification (PROCESSED) ===\")\n",
    "\n",
    "# Remove Crm Cd Desc to avoid leakage\n",
    "df_model_2 = df_new_1.copy()\n",
    "\n",
    "# Prepare training data\n",
    "X = df_model_2.drop(columns=[\"Crime_Class\"])\n",
    "y, _ = pd.factorize(df_model_2[\"Crime_Class\"])\n",
    "\n",
    "# Convert datetime columns to int64 timestamps\n",
    "for col in X.select_dtypes(include=[\"datetime\", \"datetimetz\"]).columns:\n",
    "    X[col] = X[col].view('int64')\n",
    "\n",
    "# Convert list columns to strings so they can be factorized\n",
    "for col in X.columns:\n",
    "    if X[col].apply(lambda x: isinstance(x, list)).any():\n",
    "        X[col] = X[col].astype(str)\n",
    "\n",
    "# Factorize object columns\n",
    "X = X.apply(lambda col: pd.factorize(col)[0] if col.dtype == \"object\" else col)\n",
    "\n",
    "# Fill any remaining NaN values before training\n",
    "X = X.fillna(-1)\n",
    "\n",
    "# Train-test split (stratify for class balance)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train model (Correctly named rf_model_3)\n",
    "rf_model_3 = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "rf_model_3.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = rf_model_3.predict(X_test)\n",
    "y_pred_train = rf_model_3.predict(X_train)\n",
    "\n",
    "# Evaluation\n",
    "report_test = pd.DataFrame.from_dict(\n",
    "    classification_report(y_test, y_pred, output_dict=True)\n",
    ").transpose()\n",
    "\n",
    "report_train = pd.DataFrame.from_dict(\n",
    "    classification_report(y_train, y_pred_train, output_dict=True)\n",
    ").transpose()\n",
    "\n",
    "# Accuracy scores\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Training Set Accuracy:\", train_accuracy)\n",
    "print(\"Testing Set Accuracy:\", test_accuracy)\n",
    "print(\"------------------------------------------------------------------------------------\")\n",
    "print(\"Training Set Report\")\n",
    "print(\"------------------------------------------------------------------------------------\")\n",
    "print(report_train)\n",
    "print(\"------------------------------------------------------------------------------------\")\n",
    "print(\"Testing Set Report\")\n",
    "print(\"------------------------------------------------------------------------------------\")\n",
    "print(report_test)\n",
    "print(\"------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2035e2a1",
   "metadata": {},
   "source": [
    "### Build a Confusion Matrix of Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521f0684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# Feature Importance\n",
    "# -------------------------------------------\n",
    "importance = rf_model_3.feature_importances_\n",
    "features = X.columns\n",
    "\n",
    "fi = pd.DataFrame({\"feature\": features, \"importance\": importance})\n",
    "fi.sort_values(by=\"importance\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d250459",
   "metadata": {},
   "source": [
    "### Error VS Complexity Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639e93a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Note: X, y, X_train, X_test, y_train, y_test, and rf_model_3 are defined\n",
    "# in previous cells before running this section\n",
    "\n",
    "# ================================\n",
    "# FAST STRATIFIED SUBSAMPLING (1%)\n",
    "# ================================\n",
    "# Use a tiny subset to make the curve fast while informative\n",
    "sample_ratio = 0.01\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X, y,\n",
    "    train_size=sample_ratio,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split the small subset into train/test\n",
    "X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(\n",
    "    X_small, y_small,\n",
    "    test_size=0.3,\n",
    "    stratify=y_small,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ==================================\n",
    "# MODEL COMPLEXITY VS ERROR (n_estimators)\n",
    "# ==================================\n",
    "# Vary number of trees instead of max_depth; generally faster and clearer\n",
    "n_estimators_range = [10, 20, 40, 80, 120]\n",
    "train_losses_curve = []\n",
    "test_losses_curve = []\n",
    "train_acc_curve = []\n",
    "test_acc_curve = []\n",
    "\n",
    "for n in n_estimators_range:\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n,\n",
    "        max_depth=None,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model.fit(X_train_s, y_train_s)\n",
    "\n",
    "    # Probabilities\n",
    "    train_proba = model.predict_proba(X_train_s)\n",
    "    test_proba  = model.predict_proba(X_test_s)\n",
    "\n",
    "    # Loss\n",
    "    train_losses_curve.append(log_loss(y_train_s, train_proba))\n",
    "    test_losses_curve.append(log_loss(y_test_s, test_proba))\n",
    "\n",
    "    # Accuracy\n",
    "    y_train_pred = model.predict(X_train_s)\n",
    "    y_test_pred  = model.predict(X_test_s)\n",
    "\n",
    "    train_acc_curve.append(accuracy_score(y_train_s, y_train_pred))\n",
    "    test_acc_curve.append(accuracy_score(y_test_s, y_test_pred))\n",
    "\n",
    "# ================================\n",
    "# PLOT\n",
    "# ================================\n",
    "plt.figure(figsize=(11,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(n_estimators_range, train_losses_curve, marker='o', label=\"Training Loss\")\n",
    "plt.plot(n_estimators_range, test_losses_curve, marker='o', label=\"Validation Loss\")\n",
    "plt.xlabel(\"Model Complexity (n_estimators)\")\n",
    "plt.ylabel(\"Log Loss\")\n",
    "plt.title(\"Complexity vs Error (Random Forest)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(n_estimators_range, train_acc_curve, marker='o', label=\"Training Accuracy\")\n",
    "plt.plot(n_estimators_range, test_acc_curve, marker='o', label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Model Complexity (n_estimators)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Complexity vs Accuracy (Random Forest)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9cd753",
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = range(2, 41, 2)\n",
    "train_losses_curve = []\n",
    "test_losses_curve = []\n",
    "train_acc_curve = []\n",
    "test_acc_curve = []\n",
    "\n",
    "\n",
    "for d in depths:\n",
    "    model = RandomForestClassifier(\n",
    "        max_depth=d\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Loss\n",
    "    train_proba = model.predict_proba(X_train)\n",
    "    test_proba  = model.predict_proba(X_test)\n",
    "    train_losses_curve.append(log_loss(y_train, train_proba))\n",
    "    test_losses_curve.append(log_loss(y_test, test_proba))\n",
    "\n",
    "    # Accuracy\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred  = model.predict(X_test)\n",
    "    train_acc_curve.append(accuracy_score(y_train, y_train_pred))\n",
    "    test_acc_curve.append(accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(depths, train_losses_curve, label=\"Training Loss\")\n",
    "plt.plot(depths, test_losses_curve, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Model Complexity (max_depth)\")\n",
    "plt.ylabel(\"Log Loss\")\n",
    "plt.title(\"Model Complexity vs Error (Decision Tree)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef1159b",
   "metadata": {},
   "source": [
    "### Evaluata Feature Importance (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899526ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = rf_model_3.feature_importances_\n",
    "features = X.columns\n",
    "\n",
    "fi = pd.DataFrame({\"feature\": features, \"importance\": importance})\n",
    "fi.sort_values(by=\"importance\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6828886",
   "metadata": {},
   "source": [
    "### ShuffleSplit Cross-Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396259ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "print(\"=== ShuffleSplit Cross-Validation (Random Forest) ===\")\n",
    "\n",
    "# Use the same data as the processed model (X and y from the cell above)\n",
    "\n",
    "# Create a new instance of the Random Forest Classifier with the same parameters\n",
    "cv_model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Configure ShuffleSplit\n",
    "# n_splits: Number of re-shuffling & splitting iterations.\n",
    "# test_size: Proportion of the dataset to include in the test split.\n",
    "# random_state: Ensures reproducible splits.\n",
    "shuffle_split_cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "# This will train the model 5 times on different 70/30 splits of the data.\n",
    "# It's memory-efficient because it works on splits.\n",
    "print(\"Starting cross-validation... (This may take some time depending on your machine)\")\n",
    "cv_scores = cross_val_score(cv_model, X, y, cv=shuffle_split_cv, scoring='accuracy', n_jobs=-1)\n",
    "print(\"Cross-validation finished.\")\n",
    "\n",
    "# Print the results\n",
    "print(\"\\n--- Cross-Validation Scores ---\")\n",
    "print(\"Scores for each split:\", cv_scores)\n",
    "print(f\"Mean Accuracy: {np.mean(cv_scores):.4f}\")\n",
    "print(f\"Standard Deviation: {np.std(cv_scores):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
